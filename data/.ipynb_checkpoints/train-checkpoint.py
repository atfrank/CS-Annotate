# -*- coding: utf-8 -*-
"""Copy of Kexin's version -  Structural Annotation of RNA Using NMR Chemical Shifts

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4esH0GehADUzFMJ8wNq7puJ56QufhZF

Structural Annotation of RNA Using NMR Chemical Shifts

# Manuscript Sketch

* **Introduction**:

1. RNA structure important
2. NMR probes structure
3. Chemical shifts in particular sensitive to structure
4. Highlight examples of structural parameters extracted from chemical shift data
5. Introduce the idea of structural annotation via NMR chemical shifts
  * base pairs
  * adjacent and non-adjacent stacking
  * syn vs. anti
  * solvent exposure
  * [pucker](https://www.youtube.com/watch?v=4VQxRKQcMYg)

6. Survey the use of machine learning used to predict structural parameter from NMR chemical shifts, for both proteins and RNA
7. Make point that most approaches previous approaches employed single task learning
8. Extol the virtues of multi-task learning
10. Summarize what we did and the results

* **Methods**:
1. Data set
2. MC-Annotate
3. Multi-task classifiers

* **Results**:
1. Model performance on test set (Macro-Analysis: e.g., ROC and concordance analysis) including impact of number of neighbors
2. Model performance on test set (Micro-Analysis); Compare, visually target and prediction, highlighting (and possibly explaning) errors.
3. Importance analysis (which chemical shifts are important for predicting each structural feature)

# Mount drive and load libraries
"""


# deepchem
import rdkit as rd
import deepchem as dc
import tensorflow as tf

# other
import numpy as np
import pandas as pd 
import requests
import io

"""# Functions for input preparation
1.   Updated MC-Annotate input;
2.   get_data now takes a third argument that specifies whether to use C or H or both chemical shifts;
"""

DIR_PATH = '/pylon5/mc5pksp/afrank2/cs-annotate/'
RETAIN = ['id', 'resid', 'resname', 'sasa-All-atoms', 'sasa-Total-Side', 'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All', 'sasa', 'syn_anti', 'astack', 'nastack', 'pair', 'pucker', 'class']
RETAIN_NONAME = ['id', 'resid', 'sasa-All-atoms', 'sasa-Total-Side', 'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All', 'sasa', 'syn_anti', 'astack', 'nastack', 'pair', 'pucker', 'class']

def get_cs_all(cs_all, id):
    '''
    This function gets chemical shifts for a particular RNA.
    '''
    return(cs_all[cs_all.id == id])

def get_cs_residues(cs_i, resid, dummy=0, number_of_cs_types=19):
    '''
    This function return an array contining the chemical shifts 
    for a particular residues in an RNA.
    '''
    cs_tmp=cs_i[(cs_i.resid == resid)].drop(RETAIN, axis=1)
    info_tmp=cs_i[(cs_i.resid == resid)]
    if (cs_tmp.shape[0] != 1):
        return(dummy*np.ones(shape=(1, number_of_cs_types)))
    else:
        return(cs_tmp.values)

def get_resnames(cs_i, resid, dummy="UNK"):
    '''
    This function returns the residue name for specified residue (resid)
    '''
    cs_tmp=cs_i[(cs_i.resid == resid)]
    if (cs_tmp.shape[0] != 1):
        return(dummy)
    else:
        return(cs_tmp['resname'].values[0])

def get_cs_features(cs_i, resid, neighbors, number_of_cs_types):
    '''
    This function return chemical shifts and resnames for 
    residues (resid) and its neighbors
    '''
    cs=[]
    resnames=[]
    for i in range(resid-neighbors, resid+neighbors+1):
        cs.append(get_cs_residues(cs_i, resid=i, number_of_cs_types=number_of_cs_types))
        resnames.append(get_resnames(cs_i, i))
    return(resnames, np.array(cs))

def write_out_resname(neighbors=1):
    ''' 
    Helper function that writes out the column names associated 
    resnames for a given residue and its neighbors
    '''  
    colnames = []
    for i in range(1-neighbors-1, neighbors+1):  # ['R-2', 'R-1', 'R', 'R+1', 'R+2'] when neighbors = 2
        if i < 0: 
            colnames.append('R%s'%i)
        elif i > 0: 
            colnames.append('R+%s'%i)
        else: 
            colnames.append('R')
    return(colnames)    

def get_cs_features_rna(cs, neighbors, number_of_cs_types):
    '''    
    This function generates the complete required data frame an RNA    
    '''
    all_features = []
    all_resnames = []
    for resid in cs['resid'].unique():
        resnames, features = get_cs_features(cs, resid, neighbors, number_of_cs_types)
        all_features.append(features.flatten())
        all_resnames.append(resnames)

    all_resnames = pd.DataFrame(all_resnames, dtype='object', columns = write_out_resname(neighbors))
    all_features = pd.DataFrame(all_features, dtype='object')
    info = pd.DataFrame(cs[RETAIN_NONAME].values, dtype='object', columns = RETAIN_NONAME)
    return(pd.concat([info, all_resnames, all_features], axis=1))

def get_cs_features_rna_all(cs, neighbors, number_of_cs_types):  
    '''    
    This function generate a pandas dataframe containing training data for all RNAs
    Each row in the data frame should contain the class and chemical shifts for given residue and neighbors in a given RNA.
    '''  
    cs_new=pd.DataFrame()
    for pdbid in cs['id'].unique()[0 :]:
        tmp=get_cs_features_rna(get_cs_all(cs, id=pdbid), neighbors, number_of_cs_types)
        cs_new=pd.concat([cs_new, tmp], axis=0)
    return(cs_new)
 
def one_hot_encode(df, hot_columns):
  '''
    This function generate one hot encodes a dataFrame
    see: http://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example 
  '''
  for hot_column in hot_columns:
    # use pd.concat to join the new columns with your original dataframe
    df = pd.concat([df, pd.get_dummies(df[hot_column], prefix=hot_column)],axis=1)
    # now drop the original 'country' column (you don't need it anymore)
    df.drop([hot_column],axis=1, inplace=True)
  return(df)

def balance_transformer(dataset):
    '''
      Copy of deepchem function for reweighting samples. 
      Deepchem version does not work.
    '''
    # Compute weighting factors from dataset.
    y = dataset.y
    w = dataset.w
    # Ensure dataset is binary
    np.testing.assert_allclose(sorted(np.unique(y)), np.array([0., 1.]))
    weights = []
    for ind, task in enumerate(dataset.get_task_names()):
      task_w = w[:]
      task_y = y[:]
      # Remove labels with zero weights
      task_y = task_y[task_w != 0]
      num_positives = np.count_nonzero(task_y)
      num_negatives = len(task_y) - num_positives
      if num_positives > 0:
        pos_weight = float(num_negatives) / num_positives
      else:
        pos_weight = 1
      neg_weight = 1
      weights.append((neg_weight, pos_weight)) 
    return(dc.data.NumpyDataset(X=dataset.X, y=dataset.y, w=np.where(y==1, pos_weight, neg_weight)))

def get_data(neighbors, training = True, partial = None):
  # load sasa data
  if training:    
    url="https://drive.google.com/uc?id=1Y3Imx-lTjGKCQAFqEKTbaMSzFARtwEFN&authuser=afrankz@umich.edu&usp=drive_fs"
  else:   
    url="https://drive.google.com/uc?id=1jLcowU89y4o5Xmv_qBs3VgZre5ZFFYwG&authuser=afrankz@umich.edu&usp=drive_fs"

  s=requests.get(url).content
  sasa=pd.read_csv(io.StringIO(s.decode('utf-8')), sep = "\s+")
  print("[INFO]: SASA loaded data")
  print(sasa.head())

  # load cs data
  url="https://drive.google.com/uc?id=1ApGAKHnzKUjri-f_sSPZwqK5N3Cr7gPq&authuser=afrankz@umich.edu&usp=drive_fs" 
  s=requests.get(url).content
  cs=pd.read_csv(io.StringIO(s.decode('utf-8')), sep = " ")

  # if partial = None, use both carbon and proton 
  # if partial = "C", use carbon chemical shifts only
  # if partial = "H", use proton chemical shifts only
  if partial == "C":
    cs.drop(columns=['H1p','H2p','H3p','H4p','H2','H5','H5p','H5pp','H6','H8'],inplace=True)
    number_of_cs_types = 9

  if partial == "H":
    cs.drop(columns=['C1p','C2p','C3p','C4p','C5p','C2','C5','C6','C8'],inplace=True)
    number_of_cs_types = 10

  if partial == None:
    number_of_cs_types = 19
  print("[INFO]: CS loaded data")
  #print(cs.head())

  # load MC-Annotate
  url="https://drive.google.com/uc?id=15dL0aZJZpTRxPYQn-DQ_jKC7tlH7fejg"
  s=requests.get(url).content
  mc_annotate=pd.read_csv(io.StringIO(s.decode('utf-8')), sep = " ")
  #print("[INFO]: MC-Annotate loaded data")
  
  # merge sasa and cs
  data = pd.merge(cs, sasa, on=['id', 'resname', 'resid'])
  # merge with mc-annotate structure
  data = pd.merge(data, mc_annotate, on=['id', 'resname', 'resid'])  
  #drop_names = ['sugar_puckering', 'pseudoknot', 'junk']
  #data = data.drop(drop_names, axis=1)
  print("[INFO]: merged SASA, CS, and MC-Annotate")
  #print(data.head())

    # prepare for testing
  print(data.columns)
  data_all = get_cs_features_rna_all(data, neighbors = neighbors, number_of_cs_types = number_of_cs_types)
  print("[INFO]: Prepared final data set")
  #print(data_all.head())

  data_all = one_hot_encode(data_all, write_out_resname(neighbors)) # only encode resnames (including neighbors)
  print("[INFO]: One-hot encoded data")
  #print(data_all.head())
  return(data_all)

"""# Prepare input"""

################################################################
## Prepare data
################################################################
neighbors = 3
train = get_data(neighbors, training = True, partial=None) # partial = "C" or "H" or None
test = get_data(neighbors, training = False, partial=None)

rnas_old = ['1A60', '1HWQ', '1JO7', '1KKA', '1L1W', '1LC6', '1LDZ', '1MFY',
       '1NA2', '1NC0', '1OW9', '1PJY', '1Q75', '1R7W', '1R7Z', '1SCL',
       '1SY4', '1SYZ', '1UUU', '1XHP', '1YMO', '1YSV', '1Z2J', '1Z30',
       '1ZC5', '28SP', '28SR', '2F87', '2FDT', '2GVO', '2JWV', '2JYM',
       '2K66', '2KEZ', '2KF0', '2KOC', '2KXM', '2KZL', '2L3E', '2L5Z',
       '2L8H', '2LAC', '2LBJ', '2LBK', '2LBL', '2LDL', '2LDT', '2LHP',
       '2LI4', '2LK3', '2LP9', '2LPA', '2LPS', '2LQZ', '2LUB', '2LUN',
       '2LV0', '2M12', '2M21', '2M22', '2M4W', '2M5U', '2M8K', '2MEQ',
       '2MHI', '2MIS', '2MNC', '2MTJ', '2MXL', '2N2O', '2N2P', '2N3Q',
       '2N3R', '2N4L', '2N6S', '2N6T', '2N6W', '2N6X', '2NCI', '2O33',
       '2QH2', '2QH3', '2QH4', '2RVO', '2Y95', '4A4S', '4A4T', '4A4U',
       '5A17', '5A18', '5IEM', '5KQE', '5UF3', '5UZT', '5V16', '5WQ1']
set(train['id'].unique())-set(rnas_old)
train.head()

# Prepare training set
sd_scale = 0.5
targets = ['sasa', 'astack',  'nastack', 'pair', 'syn_anti']
drop_names = ['id', 'resid', 'sasa-All-atoms', 'sasa-Total-Side', 'sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All', 'sasa', 'astack',  'nastack', 'pair', 'syn_anti', 'pucker', 'class']
tmp_trainX = train.drop(drop_names, axis=1)
tmp_trainy = pd.DataFrame(train[targets].values, dtype = 'float', columns = targets)
trainX = pd.DataFrame(tmp_trainX.values, dtype = 'float')
trainy = tmp_trainy.values
train_mean = [trainy[:, 0].mean()] # mean of sasa
train_sd = [sd_scale*np.std(trainy[:, 0])]
tmp = one_hot_encode(pd.DataFrame(train['pucker'].values, columns=['pucker']), ['pucker'])
trainy_mix = np.vstack([ np.where(trainy[:, 0] <= train_mean[0]+train_sd[0], 0, 1),   # sasa threshold                 
                        trainy[:, 1], # astack
                        trainy[:, 2], # nastack
                        trainy[:, 3], # pair
                        trainy[:, 4], # syn_anti
                        tmp['pucker_C2p_endo'].values,
                        tmp['pucker_C3p_endo'].values,
                        tmp['pucker_C2p_exo'].values,
                        tmp['pucker_C3p_exo'].values,
                        tmp['pucker_C1p_exo'].values,
                        tmp['pucker_C4p_exo'].values
                        ]).T 
w = np.ones(len(trainy[:, 0]))
train_w = np.vstack([w, w, w, w, w, w, w, w, w, w, w]).T # weight is 1 for all cells
train_dataset = dc.data.NumpyDataset(trainX, trainy_mix, train_w) # use deepchem here, target weight

# Prepare test model
retain = ['id', 'resid', 'sasa']
tmp_testX = test.drop(drop_names, axis=1)
tmp_testy = pd.DataFrame(test[targets].values, dtype = 'float', columns = targets)
targets.append('pucker_C2p_endo')
targets.append('pucker_C3p_endo')
targets.append('pucker_C2p_exo')
targets.append('pucker_C3p_exo')
targets.append('pucker_C1p_exo')
targets.append('pucker_C4p_exo')

testX = pd.DataFrame(tmp_testX.values, dtype = 'float')
testy = tmp_testy.values
tmp = one_hot_encode(pd.DataFrame(test['pucker'].values, columns=['pucker']), ['pucker'])
testy_mix = np.vstack([ np.where(testy[:, 0] <= train_mean[0]+train_sd[0], 0, 1),                         
                        testy[:, 1],
                        testy[:, 2],
                        testy[:, 3],
                        testy[:, 4],
                        tmp['pucker_C2p_endo'].values,
                        tmp['pucker_C3p_endo'].values,
                        tmp['pucker_C2p_exo'].values,
                        tmp['pucker_C3p_exo'].values,
                        tmp['pucker_C1p_exo'].values,
                        tmp['pucker_C4p_exo'].values                       
                        ]).T
w = np.ones(len(testy[:, 0]))
test_w = np.vstack([w, w, w, w, w, w, w, w, w, w, w]).T
test_dataset = dc.data.NumpyDataset(np.array(testX), np.array(testy_mix), test_w)

info = pd.DataFrame(test[retain].values, dtype='object', columns = retain) # info is id, resid, and sasa
actuals = pd.DataFrame(testy_mix, columns=targets)

col_names = ["sasa","astack","nastack","pair","syn_anti","pucker_C2p_endo","pucker_C3p_endo","pucker_C2p_exo","pucker_C3p_exo","pucker_C1p_exo","pucker_C4p_exo"]
pd.DataFrame({"sasa":trainy_mix[:, 0], 
             "astack":trainy_mix[:, 1], 
             "nastack":trainy_mix[:, 2], 
             "pair":trainy_mix[:, 3], 
             "syn_anti":trainy_mix[:, 4], 
             "pucker_C2p_endo":trainy_mix[:, 5],
             "pucker_C3p_endo":trainy_mix[:, 6],
             "pucker_C2p_exo":trainy_mix[:, 7],
             "pucker_C3p_exo":trainy_mix[:, 8],
             "pucker_C1p_exo":trainy_mix[:, 9],
             "pucker_C4p_exo":trainy_mix[:, 10]}, columns = col_names).to_csv("train_target_"+str(neighbors)+".csv", sep = " ", index = False)

trainX.to_csv(DIR_PATH+"train_features_"+str(neighbors)+".csv", sep = " ", index = False) 
pd.DataFrame({"sasa":testy_mix[:, 0], 
             "astack":testy_mix[:, 1], 
             "nastack":testy_mix[:, 2], 
             "pair":testy_mix[:, 3], 
             "syn_anti":testy_mix[:, 4], 
             "pucker_C2p_endo":testy_mix[:, 5],
             "pucker_C3p_endo":testy_mix[:, 6],
             "pucker_C2p_exo":testy_mix[:, 7],
             "pucker_C3p_exo":testy_mix[:, 8],
             "pucker_C1p_exo":testy_mix[:, 9],
             "pucker_C4p_exo":testy_mix[:, 10]}, columns = col_names).to_csv(DIR_PATH+"test_target_"+str(neighbors)+".csv", sep = " ", index = False)

testX.to_csv(DIR_PATH+"test_features_"+str(neighbors)+".csv", sep = " ", index = False)

trainy = pd.DataFrame({"sasa":trainy_mix[:, 0], 
             "astack":trainy_mix[:, 1], 
             "nastack":trainy_mix[:, 2], 
             "pair":trainy_mix[:, 3], 
             "syn_anti":trainy_mix[:, 4], 
             "pucker_C2p_endo":trainy_mix[:, 5],
             "pucker_C3p_endo":trainy_mix[:, 6],
             "pucker_C2p_exo":trainy_mix[:, 7],
             "pucker_C3p_exo":trainy_mix[:, 8],
             "pucker_C1p_exo":trainy_mix[:, 9],
             "pucker_C4p_exo":trainy_mix[:, 10]})
             
testy = pd.DataFrame({"sasa":testy_mix[:, 0], 
             "astack":testy_mix[:, 1], 
             "nastack":testy_mix[:, 2], 
             "pair":testy_mix[:, 3], 
             "syn_anti":testy_mix[:, 4], 
             "pucker_C2p_endo":testy_mix[:, 5],
             "pucker_C3p_endo":testy_mix[:, 6],
             "pucker_C2p_exo":testy_mix[:, 7],
             "pucker_C3p_exo":testy_mix[:, 8],
             "pucker_C1p_exo":testy_mix[:, 9],
             "pucker_C4p_exo":testy_mix[:, 10]})

train_dist = trainy.apply(lambda x: x.value_counts(), axis=0)
test_dist = testy.apply(lambda x: x.value_counts(), axis=0)
train_dist.to_csv(DIR_PATH+"train_targets_dist.csv", header=True, index=True, sep = " ")
test_dist.to_csv(DIR_PATH+"test_targets_dist.csv", header=True, index=True, sep = " ")

"""# Load data and preprocess"""

neighbors = 3
# load train and test data
X_train = pd.read_csv(DIR_PATH+"train_features_"+str(neighbors)+".csv",delim_whitespace=True,header=0)
y_train = pd.read_csv(DIR_PATH+"train_target_"+str(neighbors)+".csv",delim_whitespace=True,header=0)
X_test = pd.read_csv(DIR_PATH+"test_features_"+str(neighbors)+".csv",delim_whitespace=True,header=0)
y_test = pd.read_csv(DIR_PATH+"test_target_"+str(neighbors)+".csv",delim_whitespace=True,header=0)
targets = y_train.columns

# convert to deepchem dataset
w = np.ones(y_train.shape[0]) # number of samples in train
train_w = np.vstack([w, w, w, w, w, w, w, w, w, w, w]).T # weight is 1 
train_dataset = dc.data.NumpyDataset(X_train, y_train, train_w) # use deepchem here, some kind of weight

w = np.ones(y_test.shape[0]) # number of samples in test
test_w = np.vstack([w, w, w, w, w, w, w, w, w, w, w]).T # weight is 1 
test_dataset = dc.data.NumpyDataset(X_test, y_test, test_w) # use deepchem here, some kind of weight

# Scale
transform_scaler = dc.trans.transformers.NormalizationTransformer(transform_X = True, transform_y = False, dataset=train_dataset)
train_dataset_norm = transform_scaler.transform(train_dataset)
test_dataset_norm = transform_scaler.transform(test_dataset)

# Balance Dataset
transform_balancer = dc.trans.transformers.BalancingTransformer(transform_w = True, dataset=train_dataset_norm) # this will work, inconsistent names before
train_dataset_balanced = transform_balancer.transform(train_dataset_norm)

n_features = train_dataset_balanced.X.shape[1]
n_tasks = train_dataset_balanced.y.shape[1]


"""# Train, Save and Test Model"""

targets

# save model
model = dc.models.ProgressiveMultitaskClassifier(n_tasks=n_tasks,n_features=n_features,layer_sizes=[100],alpha_init_stddevs=0.04,learning_rate=0.001, model_dir=DIR_PATH+'model/', tensorboard=True, use_queue=False)
model.fit(train_dataset_balanced, nb_epoch=50)
model.get_checkpoints()

